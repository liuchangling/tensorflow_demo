{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "\n",
    "def prepare_data(df):\n",
    "    # df=df_data.drop(['name'], axis=1)  #删除name列\n",
    "    # 缺失值处理\n",
    "    age_mean = df['age'].mean()\n",
    "    df['age'] = df['age'].fillna(age_mean)# 为na的age填充均值\n",
    "    fare_mean = df['fare'].mean()\n",
    "    df['fare'] = df['fare'].fillna(fare_mean)\n",
    "    df['embarked'] = df['embarked'].fillna('S')\n",
    "    # 数值都转成int\n",
    "    df['sex'] = df['sex'].map({'male':1, 'female':0}).astype(int)\n",
    "    df['embarked'] = df['embarked'].map({'C':0, 'Q':1,'S':2}).astype(int)\n",
    "    \n",
    "    #分离特征值和标签值\n",
    "    ndarray_data = df.values\n",
    "    features = ndarray_data[:,1:]\n",
    "    label = ndarray_data[:,0]\n",
    "    \n",
    "    # 特征值标准化\n",
    "    minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "    norm_features = minmax_scale.fit_transform(features)\n",
    "    \n",
    "    return norm_features, label\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file_path = \"data/titanic3.xls\"\n",
    "df_data = pd.read_excel(data_file_path)\n",
    "\n",
    "# 筛选字段\n",
    "# 提取有用的特征字段\n",
    "selected_cols = ['survived', 'pclass','sex','age','sibsp','parch','fare','embarked']\n",
    "\n",
    "selected_df_data = df_data[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过pandas sample进行随机排序， frac 百分比 。 selected_df_data 保持不变\n",
    "shuffle_data = selected_df_data.sample(frac=1)\n",
    "x_data, y_data = prepare_data(shuffle_data)\n",
    "\n",
    "# 划分训练集和测试集，这里80%作为训练集\n",
    "train_size = int(len(x_data)*0.8)\n",
    "\n",
    "x_train = x_data[:train_size]\n",
    "y_train = y_data[:train_size]\n",
    "x_test = x_data[train_size:]\n",
    "y_test = y_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入层 - 隐藏层1 64个 - 隐藏层2 32个 - 输出层 1个神经元\n",
    "import tensorflow as tf\n",
    "# 建立Keras序列模型\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,625\n",
      "Trainable params: 2,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 这段代码比低阶api节省了好多事\n",
    "# 第一隐藏层层，输入特征数据是7列，也可用input_shape(7,)\n",
    "model.add(tf.keras.layers.Dense(units=64,  # 输出神经元数量\n",
    "                                input_dim=7, #输出数据个数\n",
    "                                use_bias=True, #启用偏置项\n",
    "                                kernel_initializer='uniform', #权重初始化方式\n",
    "                                bias_initializer='zeros', #偏置项初识为0\n",
    "                                activation='relu')) # 激活函数用relu\n",
    "\n",
    "# 第二隐藏层，这里很多都可以用缺省值。输入数据维度会由上一个层计算得到\n",
    "model.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n",
    "\n",
    "# 输出层\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "#显示模型结构\n",
    "model.summary()\n",
    "# 其中 参数个数，因为有偏置项，  (7+1)*64， (64+1) *32, (32+1)*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型设置\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.003),  # 优化器\n",
    "             loss='binary_crossentropy', # 损失函数\n",
    "             metrics=['accuracy']) # 监控度量值=准确率\n",
    "# sigmoid 作为激活函数时，损失函数常用binary_crossentropy\n",
    "# softmax 作为激活函数时，损失函数选用categorical_crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 837 samples, validate on 210 samples\n",
      "Epoch 1/100\n",
      "837/837 - 1s - loss: 0.6529 - accuracy: 0.6129 - val_loss: 0.6288 - val_accuracy: 0.6095\n",
      "Epoch 2/100\n",
      "837/837 - 0s - loss: 0.5935 - accuracy: 0.6738 - val_loss: 0.5562 - val_accuracy: 0.7810\n",
      "Epoch 3/100\n",
      "837/837 - 0s - loss: 0.5247 - accuracy: 0.7706 - val_loss: 0.5071 - val_accuracy: 0.7667\n",
      "Epoch 4/100\n",
      "837/837 - 0s - loss: 0.4946 - accuracy: 0.7766 - val_loss: 0.4946 - val_accuracy: 0.7952\n",
      "Epoch 5/100\n",
      "837/837 - 0s - loss: 0.4848 - accuracy: 0.7790 - val_loss: 0.4864 - val_accuracy: 0.7762\n",
      "Epoch 6/100\n",
      "837/837 - 0s - loss: 0.4792 - accuracy: 0.7790 - val_loss: 0.4817 - val_accuracy: 0.7810\n",
      "Epoch 7/100\n",
      "837/837 - 0s - loss: 0.4751 - accuracy: 0.7778 - val_loss: 0.4762 - val_accuracy: 0.7857\n",
      "Epoch 8/100\n",
      "837/837 - 0s - loss: 0.4720 - accuracy: 0.7826 - val_loss: 0.4748 - val_accuracy: 0.7810\n",
      "Epoch 9/100\n",
      "837/837 - 0s - loss: 0.4682 - accuracy: 0.7897 - val_loss: 0.4777 - val_accuracy: 0.7810\n",
      "Epoch 10/100\n",
      "837/837 - 0s - loss: 0.4686 - accuracy: 0.7873 - val_loss: 0.4676 - val_accuracy: 0.7905\n",
      "Epoch 11/100\n",
      "837/837 - 0s - loss: 0.4674 - accuracy: 0.7909 - val_loss: 0.4734 - val_accuracy: 0.7810\n",
      "Epoch 12/100\n",
      "837/837 - 0s - loss: 0.4678 - accuracy: 0.7838 - val_loss: 0.4653 - val_accuracy: 0.7762\n",
      "Epoch 13/100\n",
      "837/837 - 0s - loss: 0.4637 - accuracy: 0.7885 - val_loss: 0.4735 - val_accuracy: 0.8048\n",
      "Epoch 14/100\n",
      "837/837 - 0s - loss: 0.4687 - accuracy: 0.7790 - val_loss: 0.4638 - val_accuracy: 0.7857\n",
      "Epoch 15/100\n",
      "837/837 - 0s - loss: 0.4636 - accuracy: 0.7826 - val_loss: 0.4610 - val_accuracy: 0.7952\n",
      "Epoch 16/100\n",
      "837/837 - 0s - loss: 0.4613 - accuracy: 0.7897 - val_loss: 0.4599 - val_accuracy: 0.7905\n",
      "Epoch 17/100\n",
      "837/837 - 0s - loss: 0.4631 - accuracy: 0.7826 - val_loss: 0.4623 - val_accuracy: 0.8000\n",
      "Epoch 18/100\n",
      "837/837 - 0s - loss: 0.4623 - accuracy: 0.7909 - val_loss: 0.4595 - val_accuracy: 0.7857\n",
      "Epoch 19/100\n",
      "837/837 - 0s - loss: 0.4629 - accuracy: 0.7957 - val_loss: 0.4724 - val_accuracy: 0.7810\n",
      "Epoch 20/100\n",
      "837/837 - 0s - loss: 0.4632 - accuracy: 0.7873 - val_loss: 0.4582 - val_accuracy: 0.7952\n",
      "Epoch 21/100\n",
      "837/837 - 0s - loss: 0.4592 - accuracy: 0.7921 - val_loss: 0.4569 - val_accuracy: 0.8000\n",
      "Epoch 22/100\n",
      "837/837 - 0s - loss: 0.4625 - accuracy: 0.7969 - val_loss: 0.4597 - val_accuracy: 0.7762\n",
      "Epoch 23/100\n",
      "837/837 - 0s - loss: 0.4598 - accuracy: 0.7921 - val_loss: 0.4626 - val_accuracy: 0.7714\n",
      "Epoch 24/100\n",
      "837/837 - 0s - loss: 0.4616 - accuracy: 0.7909 - val_loss: 0.4549 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "837/837 - 0s - loss: 0.4594 - accuracy: 0.7873 - val_loss: 0.4539 - val_accuracy: 0.7952\n",
      "Epoch 26/100\n",
      "837/837 - 0s - loss: 0.4644 - accuracy: 0.7873 - val_loss: 0.4534 - val_accuracy: 0.8095\n",
      "Epoch 27/100\n",
      "837/837 - 0s - loss: 0.4554 - accuracy: 0.7969 - val_loss: 0.4585 - val_accuracy: 0.7857\n",
      "Epoch 28/100\n",
      "837/837 - 0s - loss: 0.4586 - accuracy: 0.7897 - val_loss: 0.4588 - val_accuracy: 0.7762\n",
      "Epoch 29/100\n",
      "837/837 - 0s - loss: 0.4619 - accuracy: 0.7814 - val_loss: 0.4574 - val_accuracy: 0.7952\n",
      "Epoch 30/100\n",
      "837/837 - 0s - loss: 0.4577 - accuracy: 0.7909 - val_loss: 0.4539 - val_accuracy: 0.8095\n",
      "Epoch 31/100\n",
      "837/837 - 0s - loss: 0.4569 - accuracy: 0.7897 - val_loss: 0.4524 - val_accuracy: 0.8143\n",
      "Epoch 32/100\n",
      "837/837 - 0s - loss: 0.4552 - accuracy: 0.7981 - val_loss: 0.4543 - val_accuracy: 0.7857\n",
      "Epoch 33/100\n",
      "837/837 - 0s - loss: 0.4599 - accuracy: 0.7849 - val_loss: 0.4557 - val_accuracy: 0.8048\n",
      "Epoch 34/100\n",
      "837/837 - 0s - loss: 0.4643 - accuracy: 0.7790 - val_loss: 0.4600 - val_accuracy: 0.8000\n",
      "Epoch 35/100\n",
      "837/837 - 0s - loss: 0.4566 - accuracy: 0.7921 - val_loss: 0.4548 - val_accuracy: 0.7857\n",
      "Epoch 36/100\n",
      "837/837 - 0s - loss: 0.4538 - accuracy: 0.7969 - val_loss: 0.4523 - val_accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "837/837 - 0s - loss: 0.4561 - accuracy: 0.7861 - val_loss: 0.4519 - val_accuracy: 0.8143\n",
      "Epoch 38/100\n",
      "837/837 - 0s - loss: 0.4597 - accuracy: 0.7957 - val_loss: 0.4731 - val_accuracy: 0.7762\n",
      "Epoch 39/100\n",
      "837/837 - 0s - loss: 0.4607 - accuracy: 0.7897 - val_loss: 0.4512 - val_accuracy: 0.7952\n",
      "Epoch 40/100\n",
      "837/837 - 0s - loss: 0.4572 - accuracy: 0.7897 - val_loss: 0.4555 - val_accuracy: 0.7905\n",
      "Epoch 41/100\n",
      "837/837 - 0s - loss: 0.4530 - accuracy: 0.7933 - val_loss: 0.4526 - val_accuracy: 0.7857\n",
      "Epoch 42/100\n",
      "837/837 - 0s - loss: 0.4517 - accuracy: 0.7957 - val_loss: 0.4511 - val_accuracy: 0.8143\n",
      "Epoch 43/100\n",
      "837/837 - 0s - loss: 0.4521 - accuracy: 0.7897 - val_loss: 0.4481 - val_accuracy: 0.8000\n",
      "Epoch 44/100\n",
      "837/837 - 0s - loss: 0.4522 - accuracy: 0.7957 - val_loss: 0.4535 - val_accuracy: 0.7810\n",
      "Epoch 45/100\n",
      "837/837 - 0s - loss: 0.4518 - accuracy: 0.7957 - val_loss: 0.4497 - val_accuracy: 0.8190\n",
      "Epoch 46/100\n",
      "837/837 - 0s - loss: 0.4542 - accuracy: 0.7933 - val_loss: 0.4499 - val_accuracy: 0.8095\n",
      "Epoch 47/100\n",
      "837/837 - 0s - loss: 0.4514 - accuracy: 0.7909 - val_loss: 0.4504 - val_accuracy: 0.8048\n",
      "Epoch 48/100\n",
      "837/837 - 0s - loss: 0.4507 - accuracy: 0.7897 - val_loss: 0.4464 - val_accuracy: 0.8000\n",
      "Epoch 49/100\n",
      "837/837 - 0s - loss: 0.4482 - accuracy: 0.7993 - val_loss: 0.4487 - val_accuracy: 0.7952\n",
      "Epoch 50/100\n",
      "837/837 - 0s - loss: 0.4488 - accuracy: 0.7969 - val_loss: 0.4481 - val_accuracy: 0.8048\n",
      "Epoch 51/100\n",
      "837/837 - 0s - loss: 0.4477 - accuracy: 0.7969 - val_loss: 0.4459 - val_accuracy: 0.8095\n",
      "Epoch 52/100\n",
      "837/837 - 0s - loss: 0.4498 - accuracy: 0.7909 - val_loss: 0.4453 - val_accuracy: 0.8095\n",
      "Epoch 53/100\n",
      "837/837 - 0s - loss: 0.4465 - accuracy: 0.8076 - val_loss: 0.4472 - val_accuracy: 0.7905\n",
      "Epoch 54/100\n",
      "837/837 - 0s - loss: 0.4463 - accuracy: 0.7969 - val_loss: 0.4421 - val_accuracy: 0.8048\n",
      "Epoch 55/100\n",
      "837/837 - 0s - loss: 0.4438 - accuracy: 0.7957 - val_loss: 0.4440 - val_accuracy: 0.8048\n",
      "Epoch 56/100\n",
      "837/837 - 0s - loss: 0.4492 - accuracy: 0.8041 - val_loss: 0.4544 - val_accuracy: 0.7762\n",
      "Epoch 57/100\n",
      "837/837 - 0s - loss: 0.4456 - accuracy: 0.7969 - val_loss: 0.4471 - val_accuracy: 0.7952\n",
      "Epoch 58/100\n",
      "837/837 - 0s - loss: 0.4439 - accuracy: 0.8029 - val_loss: 0.4425 - val_accuracy: 0.8048\n",
      "Epoch 59/100\n",
      "837/837 - 0s - loss: 0.4442 - accuracy: 0.8017 - val_loss: 0.4469 - val_accuracy: 0.8000\n",
      "Epoch 60/100\n",
      "837/837 - 0s - loss: 0.4432 - accuracy: 0.8041 - val_loss: 0.4436 - val_accuracy: 0.7952\n",
      "Epoch 61/100\n",
      "837/837 - 0s - loss: 0.4437 - accuracy: 0.7981 - val_loss: 0.4425 - val_accuracy: 0.7952\n",
      "Epoch 62/100\n",
      "837/837 - 0s - loss: 0.4428 - accuracy: 0.7921 - val_loss: 0.4434 - val_accuracy: 0.7952\n",
      "Epoch 63/100\n",
      "837/837 - 0s - loss: 0.4431 - accuracy: 0.7993 - val_loss: 0.4426 - val_accuracy: 0.8190\n",
      "Epoch 64/100\n",
      "837/837 - 0s - loss: 0.4421 - accuracy: 0.8005 - val_loss: 0.4417 - val_accuracy: 0.8048\n",
      "Epoch 65/100\n",
      "837/837 - 0s - loss: 0.4386 - accuracy: 0.7993 - val_loss: 0.4418 - val_accuracy: 0.8000\n",
      "Epoch 66/100\n",
      "837/837 - 0s - loss: 0.4400 - accuracy: 0.8005 - val_loss: 0.4434 - val_accuracy: 0.7952\n",
      "Epoch 67/100\n",
      "837/837 - 0s - loss: 0.4414 - accuracy: 0.7909 - val_loss: 0.4532 - val_accuracy: 0.7905\n",
      "Epoch 68/100\n",
      "837/837 - 0s - loss: 0.4437 - accuracy: 0.8029 - val_loss: 0.4406 - val_accuracy: 0.8000\n",
      "Epoch 69/100\n",
      "837/837 - 0s - loss: 0.4388 - accuracy: 0.7981 - val_loss: 0.4406 - val_accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "837/837 - 0s - loss: 0.4379 - accuracy: 0.7981 - val_loss: 0.4396 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "837/837 - 0s - loss: 0.4388 - accuracy: 0.7945 - val_loss: 0.4451 - val_accuracy: 0.8143\n",
      "Epoch 72/100\n",
      "837/837 - 0s - loss: 0.4439 - accuracy: 0.8005 - val_loss: 0.4420 - val_accuracy: 0.7952\n",
      "Epoch 73/100\n",
      "837/837 - 0s - loss: 0.4407 - accuracy: 0.8041 - val_loss: 0.4509 - val_accuracy: 0.7810\n",
      "Epoch 74/100\n",
      "837/837 - 0s - loss: 0.4497 - accuracy: 0.7849 - val_loss: 0.4470 - val_accuracy: 0.8000\n",
      "Epoch 75/100\n",
      "837/837 - 0s - loss: 0.4384 - accuracy: 0.8041 - val_loss: 0.4424 - val_accuracy: 0.7952\n",
      "Epoch 76/100\n",
      "837/837 - 0s - loss: 0.4378 - accuracy: 0.7885 - val_loss: 0.4422 - val_accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "837/837 - 0s - loss: 0.4376 - accuracy: 0.8065 - val_loss: 0.4404 - val_accuracy: 0.7952\n",
      "Epoch 78/100\n",
      "837/837 - 0s - loss: 0.4377 - accuracy: 0.7897 - val_loss: 0.4419 - val_accuracy: 0.8143\n",
      "Epoch 79/100\n",
      "837/837 - 0s - loss: 0.4356 - accuracy: 0.8065 - val_loss: 0.4466 - val_accuracy: 0.7952\n",
      "Epoch 80/100\n",
      "837/837 - 0s - loss: 0.4393 - accuracy: 0.8029 - val_loss: 0.4426 - val_accuracy: 0.8048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "837/837 - 0s - loss: 0.4335 - accuracy: 0.8041 - val_loss: 0.4407 - val_accuracy: 0.7952\n",
      "Epoch 82/100\n",
      "837/837 - 0s - loss: 0.4331 - accuracy: 0.8029 - val_loss: 0.4396 - val_accuracy: 0.8095\n",
      "Epoch 83/100\n",
      "837/837 - 0s - loss: 0.4333 - accuracy: 0.8076 - val_loss: 0.4396 - val_accuracy: 0.8048\n",
      "Epoch 84/100\n",
      "837/837 - 0s - loss: 0.4385 - accuracy: 0.8041 - val_loss: 0.4417 - val_accuracy: 0.8095\n",
      "Epoch 85/100\n",
      "837/837 - 0s - loss: 0.4327 - accuracy: 0.8100 - val_loss: 0.4418 - val_accuracy: 0.7952\n",
      "Epoch 86/100\n",
      "837/837 - 0s - loss: 0.4341 - accuracy: 0.8017 - val_loss: 0.4389 - val_accuracy: 0.8048\n",
      "Epoch 87/100\n",
      "837/837 - 0s - loss: 0.4345 - accuracy: 0.8041 - val_loss: 0.4398 - val_accuracy: 0.8048\n",
      "Epoch 88/100\n",
      "837/837 - 0s - loss: 0.4322 - accuracy: 0.8100 - val_loss: 0.4392 - val_accuracy: 0.8095\n",
      "Epoch 89/100\n",
      "837/837 - 0s - loss: 0.4318 - accuracy: 0.7993 - val_loss: 0.4395 - val_accuracy: 0.8143\n",
      "Epoch 90/100\n",
      "837/837 - 0s - loss: 0.4305 - accuracy: 0.8088 - val_loss: 0.4380 - val_accuracy: 0.8095\n",
      "Epoch 91/100\n",
      "837/837 - 0s - loss: 0.4319 - accuracy: 0.8076 - val_loss: 0.4409 - val_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "837/837 - 0s - loss: 0.4311 - accuracy: 0.8065 - val_loss: 0.4381 - val_accuracy: 0.8048\n",
      "Epoch 93/100\n",
      "837/837 - 0s - loss: 0.4327 - accuracy: 0.8112 - val_loss: 0.4405 - val_accuracy: 0.8048\n",
      "Epoch 94/100\n",
      "837/837 - 0s - loss: 0.4315 - accuracy: 0.8017 - val_loss: 0.4381 - val_accuracy: 0.8190\n",
      "Epoch 95/100\n",
      "837/837 - 0s - loss: 0.4321 - accuracy: 0.7981 - val_loss: 0.4392 - val_accuracy: 0.8095\n",
      "Epoch 96/100\n",
      "837/837 - 0s - loss: 0.4342 - accuracy: 0.8100 - val_loss: 0.4376 - val_accuracy: 0.8000\n",
      "Epoch 97/100\n",
      "837/837 - 0s - loss: 0.4309 - accuracy: 0.8100 - val_loss: 0.4383 - val_accuracy: 0.8095\n",
      "Epoch 98/100\n",
      "837/837 - 0s - loss: 0.4327 - accuracy: 0.8076 - val_loss: 0.4401 - val_accuracy: 0.8000\n",
      "Epoch 99/100\n",
      "837/837 - 0s - loss: 0.4288 - accuracy: 0.8124 - val_loss: 0.4392 - val_accuracy: 0.8095\n",
      "Epoch 100/100\n",
      "837/837 - 0s - loss: 0.4287 - accuracy: 0.8124 - val_loss: 0.4389 - val_accuracy: 0.8095\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "train_history = model.fit(x = x_train,\n",
    "                          y = y_train,\n",
    "                          validation_split = 0.2, # 8/2开拆分样本集和测试集，验证集所占比例\n",
    "                          epochs = 100,\n",
    "                          batch_size = 40,\n",
    "                          verbose = 2)  # 训练过程 0 - 不显示,1 - 进度条  ,2 - 每个epoch显示一行\n",
    "\n",
    "\n",
    "# 返回值是训练过程的loss和acc数据，以及验证过程（如有）\n",
    "# 可以通过.history 查看\n",
    "# print(train_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 29us/sample - loss: 0.2560 - accuracy: 0.8282\n",
      "[0.371098417703432, 0.82824427]\n",
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "# 模型评估\n",
    "evaluate_result = model.evaluate(x=x_test, y=y_test)\n",
    "\n",
    "print(evaluate_result)\n",
    "\n",
    "print(model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass  sex   age  sibsp  parch   fare  embarked  \\\n",
      "0         0       3    1  23.0      1      0    5.0         2   \n",
      "1         1       1    0  20.0      1      0  100.0         2   \n",
      "\n",
      "   surv_probability  \n",
      "0          0.127346  \n",
      "1          0.976578  \n"
     ]
    }
   ],
   "source": [
    "# 模型预测\n",
    "Jack_info = [0, 3, 'male', 23, 1, 0, 5.0, 'S']\n",
    "Rose_info = [1, 1, 'female', 20, 1, 0, 100.0, 'S']\n",
    "\n",
    "# 创建旅客DataFrame\n",
    "new_passenger_pd = pd.DataFrame([Jack_info, Rose_info], columns=selected_cols)\n",
    "\n",
    "# 在老的DataFrame中加入新旅客信息，即数据汇总\n",
    "all_passenger_pd = selected_df_data.append(new_passenger_pd)\n",
    "\n",
    "# 数据预处理\n",
    "x_features,y_label = prepare_data(all_passenger_pd)\n",
    "\n",
    "# 利用模型计算旅客生存概率\n",
    "surv_probability = model.predict(x_features)\n",
    "\n",
    "# print(surv_probability[:5])\n",
    "\n",
    "#在数据表中插入生存概率\n",
    "all_passenger_pd.insert(len(all_passenger_pd.columns), 'surv_probability', surv_probability)\n",
    "print(all_passenger_pd[-2:])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
