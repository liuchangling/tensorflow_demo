<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 第一章 引言
人工智能真正的挑战在于解决那些对人来说很容易执行，但很难形式化描述的任务，如识别人们所说的话或图像中的脸。对于这些问题，人类往往可以凭借直觉解决。
硬编码知识工作量过大，AI系统需要具备自己获取知识的能力。即从原始数据中提取模式的能力。这种能力称之为**机器学习**
**深度学习的过程** 输入-简单特征-更抽象特征的额外层-从特征映射-输出
# 第二章 线性代数
**逆矩阵**与A相乘为单位矩阵

**线性相关**说白了就是无法表达全空间的，一般是某几个可以合成其中一个

**线性无关**就是能组合到全空间的

**范数** 
$$ ||x|| = (\sum_i |x_i|^p)^{1 \over p} $$
L2又被称为欧几里得范数
向量相乘为0又称为**正交**

**迹运算**即矩阵对角线之和

**行列式**矩阵参与乘法后，空间扩大或者缩小了多少。 记 det(A)

# 第三章 概率与信息论
**归一化** normalized 即所有事情发生概率之和为1

**期望** 
$$ E[f(x)] = \sum_x P(x)f(x) $$

**方差** 根据概率采样时，x会呈现多大差异
$$ var(f(x)) = E[(f(x) - E[f(x)]^2] $$

**协方差** 两个变量线性相关性的强度
$$ Cov(f(x), g(y)) = E[ (f(x) - E[f(x)]) (g(x) - E[g(y)])  ] $$

**协方差矩阵**
$$ Cov(x)_{i,j} = Cov(x_i, y_j) $$

## 贝叶斯规则
$$ P(x|y) = {P(x)P(y|x) \over P(y)} $$


# 第四章 数值计算
softmax存在的重大意义：防止下溢

我名字真的多：需要最大化或者最小化的函数称为**目标函数**，或者**准则**
当我们对其最小化时，也把他称为**代价函数**或者**损失函数**或者**误差函数**

## 梯度下降
沿着导数反方向直到最小

## 学习率
一个步长的标量。 乘以坡度得出移动距离

**极小点** f'(x) = 0 且 f''(x) > 0

# 第五章 机器学习基础
任务T 性能度量P 经验E




