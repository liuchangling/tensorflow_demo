<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
## 单神经元模型
输入数据 - 求和 - 激活函数
$$ output = f(z) = f(\sum_{i=1}^n( x_i * w_i + b)) $$

### 常见的激活函数
- Sigmoid S型函数
$$  \sigma (x) = {{1} \over {1+ e ^{-x}}} $$
- tanh 双曲正切函数
$$ tanh(x) $$
- ReLu 修正线性单元函数
$$ max(0,x) $$

## 全连接单隐含层神经网络
就是在输入层和输出层之间加了一个隐藏层

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

**全连接** 在神经网络中，相邻层所有节点都有链接，且没有跨层链接。
求和之后一般要用激活函数处理一下输出（归一化）

### 常见激活函数
sigmoid tanh relu softmax

单层神经网络和多层神经网络其实原理都差不多。
要注意一点，如果直接forward 用softmax 然后用交叉熵计算loss_function
会出现log(0)的无限大loss error。
解决办法也很简单，调用tf.nn.softmax_cross_entropy_with_logits 计算loss即可

## 保存与恢复
保存
```python 
import os 
ckpt_dir = './ckpt_dir/'
if not os.path.exists(ckpt_dir):
    os.makedirs(ckpt_dir)
#....
# 【新增】存储模型的粒度
save_step = 5
# 所有变量声明完之后，调用tf.train.Saver()
saver = tf.train.Saver()

#....

    if(epoch+1)% save_step == 0: 
            saver.save(sess, os.path.join(ckpt_dir, 'mnist_h256_model_{:06d}.ckpt'.format(epoch+1)))
            
saver.save(sess, os.path.join(ckpt_dir, 'mnist_h256_model.ckpt'))
print("model saved")
```

恢复
``` python
ckpt = tf.train.get_checkpoint_state(ckpt_dir)
```
